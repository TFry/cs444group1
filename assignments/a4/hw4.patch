diff -urN original/arch/x86/syscalls/syscall_32.tbl modified/arch/x86/syscalls/syscall_32.tbl
--- original/arch/x86/syscalls/syscall_32.tbl	2017-06-08 22:04:19.878638000 -0700
+++ modified/arch/x86/syscalls/syscall_32.tbl	2017-06-08 11:07:37.130497000 -0700
@@ -359,3 +359,5 @@
 350	i386	finit_module		sys_finit_module
 351	i386	sched_setattr		sys_sched_setattr
 352	i386	sched_getattr		sys_sched_getattr
+353	i386	slob_units_used		sys_slob_units_used
+354	i386	slob_units_free		sys_slob_units_free
diff -urN original/include/linux/syscalls.h modified/include/linux/syscalls.h
--- original/include/linux/syscalls.h	2017-05-05 21:22:16.736526000 -0700
+++ modified/include/linux/syscalls.h	2017-06-08 11:06:24.451649000 -0700
@@ -855,4 +855,7 @@
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
 asmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);
+
+asmlinkage long sys_slob_units_used(void);
+asmlinkage long sys_slob_units_free(void);
 #endif
diff -urN original/mm/slob.c modified/mm/slob.c
--- original/mm/slob.c	2017-05-14 22:23:34.050032000 -0700
+++ modified/mm/slob.c	2017-06-08 21:43:53.330541000 -0700
@@ -71,8 +71,12 @@
 #include <trace/events/kmem.h>
 
 #include <linux/atomic.h>
+#include <linux/linkage.h>
+#include <linux/syscalls.h>
 
 #include "slab.h"
+
+unsigned long pages;
 /*
  * slob_block has a field 'units', which indicates size of block if +ve,
  * or offset of next block if -ve (in SLOB_UNITs).
@@ -267,12 +271,13 @@
  */
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
-	struct page *sp;
+    	struct page *sp, *cur, *best = NULL;
 	struct list_head *prev;
-	struct list_head *slob_list;
+	struct list_head *slob_list, *iter;
 	slob_t *b = NULL;
 	unsigned long flags;
-
+	int i, units;
+	
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
 	else if (size < SLOB_BREAK2)
@@ -291,13 +296,32 @@
 		if (node != NUMA_NO_NODE && page_to_nid(sp) != node)
 			continue;
 #endif
+		iter = slob_list;
+		cur = best = sp;
+        	
+
 		/* Enough room on this page? */
 		if (sp->units < SLOB_UNITS(size))
 			continue;
+        	// keep track of best fit. number is arbitrary but helps this run in a reasonable amount of time
+		for(i = 0; i < 50; i++)
+		{
+			units = list_entry(iter->next,struct page,list)->units;
+			if (units >= SLOB_UNITS(size) && units < best->units) 
+			{
+				best = list_entry(iter->next,struct page,list);
+			}
+			
+			// break if we've circled back around
+			if(list_entry(iter->next,struct page,list) == cur)
+				break;
+			
+			iter = iter->next;
+		}
 
 		/* Attempt to alloc */
-		prev = sp->list.prev;
-		b = slob_page_alloc(sp, size, align);
+		prev = best->list.prev;
+		b = slob_page_alloc(best, size, align);
 		if (!b)
 			continue;
 
@@ -328,6 +352,7 @@
 		b = slob_page_alloc(sp, size, align);
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
+		pages++;
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
@@ -362,6 +387,8 @@
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
+		pages--;
+		
 		return;
 	}
 
@@ -643,3 +670,47 @@
 {
 	slab_state = FULL;
 }
+
+unsigned long free_units(void)
+{
+	unsigned long total = 0;
+	struct page *sp;
+	struct list_head *iter;
+	unsigned long flags;
+	spin_lock_irqsave(&slob_lock, flags);
+
+	iter = &free_slob_small;
+	list_for_each_entry(sp, iter, list){
+
+		total += sp->units;	
+	
+	}
+
+	iter = &free_slob_medium;
+	list_for_each_entry(sp, iter, list){
+
+		total += sp->units;	
+	
+	}
+
+	iter = &free_slob_large;
+	list_for_each_entry(sp, iter, list){
+
+		total += sp->units;	
+	
+	}
+
+	spin_unlock_irqrestore(&slob_lock, flags);
+
+	return total;
+}
+
+asmlinkage long sys_slob_units_used(void)
+{
+	return (SLOB_UNITS(PAGE_SIZE) * pages) - free_units();
+}
+
+asmlinkage long sys_slob_units_free(void)
+{
+	return free_units();
+}
